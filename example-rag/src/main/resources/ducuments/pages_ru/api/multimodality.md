# Multimodality API

// ![Orbis Sensualium Pictus, align="center"](orbis-sensualium-pictus2.jpg)

> "Все вещи, которые естественно связаны, должны преподаваться в сочетании" - Ян Амос Коменский, "Orbis Sensualium Pictus", 1658

Люди обрабатывают знания одновременно через несколько режимов ввода данных. 
То, как мы учимся, наши переживания — все это мультимодально. 
У нас нет только зрения, только звука и только текста.

Вопреки этим принципам, машинное обучение часто сосредотачивалось на специализированных моделях, предназначенных для обработки одной модальности. 
Например, мы разработали аудиомодели для задач, таких как преобразование текста в речь или распознавание речи, и модели компьютерного зрения для задач, таких как обнаружение и классификация объектов.

Однако начинается новая волна мультимодальных больших языковых моделей. 
Примеры включают GPT-4o от OpenAI, Vertex AI Gemini 1.5 от Google, Claude3 от Anthropic, а также открытые решения Llama3.2, LLaVA и BakLLaVA, которые могут принимать несколько входов, включая текст, изображения, аудио и видео, и генерировать текстовые ответы, интегрируя эти входы.

> **Примечание:** Возможности мультимодальной большой языковой модели (LLM) позволяют моделям обрабатывать и генерировать текст в сочетании с другими модальностями, такими как изображения, аудио или видео.

## Spring AI MultimodalityMultimodality относится к способности модели одновременно понимать и обрабатывать информацию из различных источников, включая текст, изображения, аудио и другие форматы данных.

Spring AI Message API предоставляет все необходимые абстракции для поддержки мультимодальных LLM.

![Spring AI Message API, width=800, align="center"](spring-ai-message-api.jpg)

Поле `content` класса UserMessage используется в первую очередь для текстовых вводов, в то время как необязательное поле `media` позволяет добавлять одно или несколько дополнительных содержаний различных модальностей, таких как изображения, аудио и видео. 
`MimeType` указывает тип модальности. 
В зависимости от используемых LLM, поле данных `Media` может быть либо сырым медиа-контентом в виде объекта `Resource`, либо `URI` на контент.

> **Примечание:** Поле media в настоящее время применимо только для сообщений ввода пользователя (например, `UserMessage`). Оно не имеет значения для системных сообщений. `AssistantMessage`, который включает ответ LLM, предоставляет только текстовое содержимое. Для генерации не текстовых медиа-выходов вы должны использовать одну из специализированных моделей с одной модальностью.*

Например, мы можем взять следующее изображение (`multimodal.test.png`) в качестве ввода и попросить LLM объяснить, что он видит.

![Multimodal Test Image, 200, 200, align="left"](multimodal.test.png)

Для большинства мультимодальных LLM код Spring AI будет выглядеть примерно так:

```java
var imageResource = new ClassPathResource("/multimodal.test.png");

var userMessage = UserMessage.builder()
    .text("Объясните, что вы видите на этом изображении?") // content
    .media(new Media(MimeTypeUtils.IMAGE_PNG, this.imageResource)) // media
    .build();

ChatResponse response = chatModel.call(new Prompt(this.userMessage));
```

или с использованием API xref::api/chatclient.adoc[ChatClient]:

```java
String response = ChatClient.create(chatModel).prompt()
		.user(u -> u.text("Объясните, что вы видите на этом изображении?")
				    .media(MimeTypeUtils.IMAGE_PNG, new ClassPathResource("/multimodal.test.png")))
		.call()
		.content();
```

и получить ответ, например:

> Это изображение фруктовой миски с простым дизайном. Миска сделана из металла с изогнутыми проволочными краями, которые создают открытую структуру, позволяя фруктам быть видимыми со всех сторон. Внутри миски находятся два желтых банана, лежащих на том, что, похоже, является красным яблоком. Бананы слегка перезрелые, о чем свидетельствуют коричневые пятна на их кожуре. У миски есть металлическое кольцо сверху, вероятно, для удобства переноски. Миска стоит на ровной поверхности с нейтральным фоном, который обеспечивает четкий вид на фрукты внутри.

Spring AI предоставляет мультимодальную поддержку для следующих моделей чата:

- xref:api/chat/anthropic-chat.adoc#_multimodal[Anthropic Claude 3]
- xref:api/chat/bedrock-converse.adoc#_multimodal[AWS Bedrock Converse]
- xref:api/chat/azure-openai-chat.adoc#_multimodal[Azure Open AI (например, модели GPT-4o)]
- xref:api/chat/mistralai-chat.adoc#_multimodal[Mistral AI (например, модели Mistral Pixtral)]
- xref:api/chat/ollama-chat.adoc#_multimodal[Ollama (например, модели LLaVA, BakLLaVA, Llama3.2)]
- xref:api/chat/openai-chat.adoc#_multimodal[OpenAI (например, модели GPT-4 и GPT-4o)]
- xref:api/chat/vertexai-gemini-chat.adoc#_multimodal[Vertex AI Gemini (например, модели gemini-1.5-pro-001, gemini-1.5-flash-001)]

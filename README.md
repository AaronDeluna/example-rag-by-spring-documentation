# Example RAG + MCP Project

## О проекте
Этот проект демонстрирует связку:
- **RAG (Retrieval-Augmented Generation)**: ответы LLM на вопросы с опорой на локальную документацию.
- **MCP (Model Context Protocol)**: управляемая работа агента с контекстом, файлами и инструментами.

В качестве базы знаний используется документация по вымышленному **Lilipup Framework** в папке `src/main/resources/ducuments`.

## Что сейчас в репозитории
- Модульная документация (`core`, `configuration`, `lifecycle`, `validation`, `kafka`, `security`, `scheduler`, `monitoring`, `overview`).
- Тестовый набор вопросов: `rag_questions.tsv`.
- Сырые ответы эндпоинта: `rag_results.tsv`.

## Первая итерация прогона RAG
Тестировался эндпоинт:
- `GET http://localhost:8080/llm/serch-rag?questions={вопрос}`

Объем прогона:
- **50 вопросов** (30 позитивных с ожидаемым ответом из документации + 20 негативных, где ответа в контексте нет).

Результаты:
- **Позитивные:** 19/30 корректно или частично корректно.
- **Позитивные с проблемами:** 11/30 (ложные отказы, неточности, таймауты).
- **Негативные:** 18/20 корректный отказ (модель сообщает об отсутствии информации).
- **Галлюцинации на негативных:** 2/20.
- **Таймауты эндпоинта:** 2/50 запросов.

Итоговая оценка первой итерации:
- **6/10** — рабочий черновой уровень, но требуется улучшение retrieval и стабильности ответа.

## Основные выводы
- Базовые факты из документации извлекаются, но не всегда стабильно.
- Есть ложные отказы даже при наличии ответа в документации.
- Нужны доработки ранжирования/поиска и обработка таймаутов.
